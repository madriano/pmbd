{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020/21**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Recommender Systems\n",
    "This lecture is primarily about recommender systems. Also, we highlight the usefulness of Spark SQL, particularly when it relates to persistent tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spark SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As mentioned in the initial lectures, Spark SQL is a Spark module for structured data processing. It works alongside the APIs of DataFrame and Dataset and it is responsible for performing extra optimizations. \n",
    "We can also execute SQL queries and reading data from various files formats an Hive tables. (Apache Hive can manage large datasets residing in distributed storage using SQL) \n",
    "\n",
    "Further details can be found in https://spark.apache.org/docs/latest/sql-programming-guide.html and \n",
    "https://spark.apache.org/docs/latest/api/sql/index.html\n",
    "\n",
    "We can check the reference guide for Structured Query Language (SQL) which includes syntax, semantics, keywords, and examples for common SQL usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "This exercise is about to build a **recommender system of books**, with focus on the recommendation model itself.\n",
    "\n",
    "The functional requirements for the Spark program we want to create are as follows:\n",
    "\n",
    "\n",
    "1. To load the dataset of interest and perform exploratory analysis, then store the information, including as SQL tables.\n",
    "2. To create a recommendation model supported by the Spark's ALS algorithm\n",
    "3. To pre-compute recommendations and store them as SQL tables.\n",
    "4. To show recommendations of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Context\n",
    "\n",
    "The data we are processing is from the dataset **Book-Crossing**. As stated in the website from where it can be downloaded, http://www2.informatik.uni-freiburg.de/~cziegler/BX/, the BookCrossing (BX) dataset was collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the Book-Crossing community with kind permission from Ron Hornbaker, CTO of Humankind Systems. It contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit / implicit) about 271,379 books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T19:09:50.951636Z",
     "start_time": "2021-03-06T19:09:50.937311Z"
    },
    "hidden": true
   },
   "source": [
    "The `Book-Crossing` dataset comprises 3 tables, as follows:\n",
    "\n",
    "- **BX-Users**.\n",
    "Contains the users. Note that user IDs (`User-ID`) have been anonymized and map to integers. Demographic data is provided (`Location`, `Age`) if available. Otherwise, these fields contain NULL-values.\n",
    "\n",
    "- **BX-Books**.\n",
    "Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given (`Book-Title`, `Book-Author`, `Year-Of-Publication`, `Publisher`), obtained from Amazon Web Services. Note that in case of several authors, only the first is provided. URLs linking to cover images are also given, appearing in three different flavours (`Image-URL-S`, `Image-URL-M`, `Image-URL-L`), i.e., small, medium, large. These URLs point to the Amazon web site.\n",
    "\n",
    "- **BX-Book-Ratings**.\n",
    "Contains the book rating information. Ratings (`Book-Rating`) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0.\n",
    "\n",
    "\n",
    "The columns are separated by `;` and all files contain the correspondent header."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:42.638456Z",
     "start_time": "2021-04-22T15:24:42.116982Z"
    }
   },
   "outputs": [],
   "source": [
    "# basic imports \n",
    "\n",
    "import os # OS e.g directory structure\n",
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import scipy as sc  # scientific computing\n",
    "import pandas as pd # data processing, file I/O\n",
    "import seaborn as sns  # visualization\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:42.673784Z",
     "start_time": "2021-04-22T15:24:42.640052Z"
    }
   },
   "outputs": [],
   "source": [
    "# Spark related imports\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "To load the dataset of interest and perform exploratory analysis, then store the information in a SQL table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T19:02:25.357775Z",
     "start_time": "2021-04-20T19:02:25.353618Z"
    }
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:45.264170Z",
     "start_time": "2021-04-22T15:24:44.258031Z"
    }
   },
   "outputs": [],
   "source": [
    "! pwd \n",
    "! ls -la\n",
    "! head -n 3 BX-Users.csv\n",
    "! tail -n 3 BX-Users.csv\n",
    "! head -n 3 BX-Books.csv\n",
    "! tail -n 3 BX-Books.csv\n",
    "! head -n 3 BX-Book-Ratings.csv\n",
    "! tail -n 3 BX-Book-Ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:49.703289Z",
     "start_time": "2021-04-22T15:24:45.266097Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "# this time we will be using pyspark.sql.DataFrameReader.csv\n",
    "\n",
    "df_users = spark.read.csv(\"BX-Users.csv\", header=\"true\", inferSchema=\"true\", sep=\";\")\n",
    "df_books = spark.read.csv(\"BX-Books.csv\", header=\"true\", inferSchema=\"true\", sep=\";\")\n",
    "df_ratings = spark.read.csv(\"BX-Book-Ratings.csv\", header=\"true\", inferSchema=\"true\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis\n",
    "\n",
    "Let us get some insight from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:50.143288Z",
     "start_time": "2021-04-22T15:24:49.704840Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the users, both schema and some content\n",
    "\n",
    "df_users.printSchema()\n",
    "df_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:50.249700Z",
     "start_time": "2021-04-22T15:24:50.145068Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:50.433246Z",
     "start_time": "2021-04-22T15:24:50.251488Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the books, both schema and some content\n",
    "\n",
    "df_books.printSchema()\n",
    "df_books.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:50.616383Z",
     "start_time": "2021-04-22T15:24:50.435882Z"
    }
   },
   "outputs": [],
   "source": [
    "df_books.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:50.845104Z",
     "start_time": "2021-04-22T15:24:50.617965Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the ratings, both schema and some content\n",
    "\n",
    "df_ratings.printSchema()\n",
    "df_ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:50.951046Z",
     "start_time": "2021-04-22T15:24:50.848527Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ratings.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleasing/preparation\n",
    "Based on the initial reading of data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:51.365941Z",
     "start_time": "2021-04-22T15:24:50.952530Z"
    }
   },
   "outputs": [],
   "source": [
    "# check Age column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of NULL (as string) in the Age column, a little bit less than half. So, we may\n",
    "\n",
    "- replace the NULLs with the average of others\n",
    "- drop the column Age in case we can live without it\n",
    "- delete the records with NULL in the column Age\n",
    "\n",
    "It is an open discussion ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:51.798755Z",
     "start_time": "2021-04-22T15:24:51.790590Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:54.020459Z",
     "start_time": "2021-04-22T15:24:52.650400Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking nulls\n",
    "\n",
    "[df_users.dropna().count(), df_books.dropna().count(), df_ratings.dropna().count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:55.401480Z",
     "start_time": "2021-04-22T15:24:54.935477Z"
    }
   },
   "outputs": [],
   "source": [
    "[df_users.count(), df_books.count(), df_ratings.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:24:57.866663Z",
     "start_time": "2021-04-22T15:24:57.055986Z"
    }
   },
   "outputs": [],
   "source": [
    "# summary of basic statistics about numerical columns with describe()\n",
    "\n",
    "df_books.\n",
    "df_ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:00.480989Z",
     "start_time": "2021-04-22T15:24:59.462427Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2004 was when data was collected\n",
    "\n",
    "df_books.select\n",
    "df_books.select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:02.324114Z",
     "start_time": "2021-04-22T15:25:01.533981Z"
    }
   },
   "outputs": [],
   "source": [
    "# check prior to 1900\n",
    "\n",
    "df_books.select\n",
    "df_books.select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:02.716725Z",
     "start_time": "2021-04-22T15:25:02.567519Z"
    }
   },
   "outputs": [],
   "source": [
    "# check with different years of publication ... 0, 1378, 2031, etc.\n",
    "\n",
    "df_books.select('Book-Title', 'Book-Author', 'Year-Of-Publication').where(col('Year-Of-Publication')==0).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we may take cleansing data further ... we leave it as an exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:04.714918Z",
     "start_time": "2021-04-22T15:25:04.709424Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will drop some columns anyway\n",
    "\n",
    "df_books = df_books.drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is a problem with ISBN: we need it as a number so we can use it to build the model!\n",
    "\n",
    "We are going to use StringIndexer for that matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:09.785766Z",
     "start_time": "2021-04-22T15:25:06.865496Z"
    }
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"ISBN\", outputCol=\"ISBN-Index\")\n",
    "df_ratings_indexed = indexer.fit(df_ratings).transform(df_ratings)\n",
    "df_ratings_indexed.select('ISBN','ISBN-Index').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is clean, save it to files so we can use them to build models when we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:10.779367Z",
     "start_time": "2021-04-22T15:25:09.787798Z"
    }
   },
   "outputs": [],
   "source": [
    "output_users = \"users.parquet\"\n",
    "\n",
    "df_users.write.mode(\"overwrite\").parquet(output_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:11.640082Z",
     "start_time": "2021-04-22T15:25:10.781525Z"
    }
   },
   "outputs": [],
   "source": [
    "output_books = \"books.parquet\"\n",
    "\n",
    "df_books.write.mode(\"overwrite\").parquet(output_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:15.833786Z",
     "start_time": "2021-04-22T15:25:11.641560Z"
    }
   },
   "outputs": [],
   "source": [
    "output_ratings = \"ratings.parquet\"\n",
    "\n",
    "df_ratings_indexed.write.mode(\"overwrite\").parquet(output_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, save them as persistent tables into Hive metastore\n",
    "\n",
    "**Notice** \n",
    "\n",
    "- An existing Hive deployment is not necessary to use this feature. Spark will take care of it.\n",
    "- We can create a SQL table from a DataFrame with *createOrReplaceTempView* command, valid for the session. (there is also the option of global temporary views, to be shared among all sessions till the Spark application terminates)\n",
    "- But with *saveAsTable*, there will be a pointer to the data in the Hive metastore. So persistent tables will exist even after the Spark program has restarted, as long as connection is maintained to the same metastore. \n",
    "\n",
    "See details in http://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:33.771986Z",
     "start_time": "2021-04-22T15:25:22.864381Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users.write.mode(\"overwrite\").saveAsTable(\"UsersTable\")\n",
    "df_books.write.mode(\"overwrite\").saveAsTable(\"BooksTable\")\n",
    "df_ratings_indexed.write.mode(\"overwrite\").saveAsTable(\"RatingsTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:33.775387Z",
     "start_time": "2021-04-22T15:25:33.773567Z"
    }
   },
   "outputs": [],
   "source": [
    "# get rid of dataframes no longer needed\n",
    "\n",
    "del df_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T12:46:42.492329Z",
     "start_time": "2021-03-06T12:46:42.487767Z"
    }
   },
   "source": [
    "## Recommendation model\n",
    "\n",
    "In order to create the recommendation model, we will use the Alternating Least Squares (ALS) algorithm provided by Spark MLlib. See details in \n",
    "http://spark.apache.org/docs/latest/ml-collaborative-filtering.html, as we advise to check the main assumptions the implemented algorithm relies upon. For example, notice that:\n",
    "\n",
    "- it underlies a collaborative filtering strategy;\n",
    "- it aims to fill in the missing entries of a user-item association matrix, in which users and items are described by a small set of latent factors that can be used to predict missing entries. The latent factors are learned by the ALS algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/testing data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:38.825899Z",
     "start_time": "2021-04-22T15:25:34.787351Z"
    }
   },
   "outputs": [],
   "source": [
    "dftrain, dftest = df_ratings_indexed.randomSplit([0.8, 0.2], 42)\n",
    "\n",
    "# caching data ... but just the train\n",
    "dftrain.cache()\n",
    "\n",
    "# print the number of rows in each part\n",
    "[dftrain.count(), dftest.count(), df_ratings_indexed.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:25:38.833048Z",
     "start_time": "2021-04-22T15:25:38.827585Z"
    }
   },
   "outputs": [],
   "source": [
    "# recalling the schema of training data\n",
    "\n",
    "dftrain.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:26:00.599042Z",
     "start_time": "2021-04-22T15:25:38.834910Z"
    }
   },
   "outputs": [],
   "source": [
    "# build the recommendation model using ALS on the training data\n",
    "# note that we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "\n",
    "als = ALS(maxIter=5, \n",
    "          regParam=0.01, \n",
    "          userCol=\"User-ID\", \n",
    "          itemCol=\"ISBN-Index\", \n",
    "          ratingCol=\"Book-Rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# if the rating matrix is derived from another source of information \n",
    "# (i.e. it is inferred from other signals), we may set implicitPrefs \n",
    "# to True to get better results (see ALS reference)\n",
    "\n",
    "# fit the model\n",
    "\n",
    "model = als.fit(dftrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:26:45.923758Z",
     "start_time": "2021-04-22T15:26:00.600845Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the model by computing the RMSE on the test data\n",
    "\n",
    "predictions = model.transform(dftest)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", \n",
    "                                labelCol=\"Book-Rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:26:48.632612Z",
     "start_time": "2021-04-22T15:26:45.925739Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the ALS model for further use if required\n",
    "\n",
    "modelpath = \"ALSmodel\"\n",
    "model.write().overwrite().save(modelpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:26:48.746905Z",
     "start_time": "2021-04-22T15:26:48.633953Z"
    }
   },
   "outputs": [],
   "source": [
    "# just checking the files\n",
    "\n",
    "! ls -la ./ALSmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-computing recommendations and store as persistent tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:26:48.781158Z",
     "start_time": "2021-04-22T15:26:48.751181Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all users and books\n",
    "\n",
    "users = df_ratings_indexed.\n",
    "books = df_ratings_indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:27:10.827541Z",
     "start_time": "2021-04-22T15:26:48.794164Z"
    }
   },
   "outputs": [],
   "source": [
    "users.show()\n",
    "books.show()\n",
    "[users.count(), books.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:27:11.043047Z",
     "start_time": "2021-04-22T15:27:10.831218Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate top book recommendations for each user\n",
    "\n",
    "n_topbooks = 5\n",
    "user_recs = model.recommendForAllUsers(n_topbooks)\n",
    "\n",
    "# generate top user recommendations for each book\n",
    "\n",
    "#n_topusers = 5\n",
    "# book_recs = model.recommendForAllItems(n_topusers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:32:47.310129Z",
     "start_time": "2021-04-22T15:27:11.044275Z"
    }
   },
   "outputs": [],
   "source": [
    "user_recs.show()\n",
    "# book_recs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recommendations as persistent tables into the Hive metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:38:47.125218Z",
     "start_time": "2021-04-22T15:32:47.311731Z"
    }
   },
   "outputs": [],
   "source": [
    "user_recs.write.mode(\"overwrite\").saveAsTable(\"UserRecommendationsTable\")\n",
    "# book_recs.write.mode(\"overwrite\").saveAsTable(\"BookRecommendationsTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "1. Give a user, shows the recommended list of books.\n",
    "2. Given a book, shows the list of users might be interested on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:38:47.211472Z",
     "start_time": "2021-04-22T15:38:47.209316Z"
    }
   },
   "outputs": [],
   "source": [
    "user = 1238\n",
    "# book = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let use check the SQL tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:43:48.904023Z",
     "start_time": "2021-04-22T15:43:48.875642Z"
    }
   },
   "outputs": [],
   "source": [
    "# register information about users and books as SQL temporary views\n",
    "\n",
    "df_users.\n",
    "df_books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:43:50.205702Z",
     "start_time": "2021-04-22T15:43:50.144835Z"
    }
   },
   "outputs": [],
   "source": [
    "print(spark.catalog.listDatabases())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:42:19.152992Z",
     "start_time": "2021-04-22T15:42:18.914624Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.listTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:52:32.633999Z",
     "start_time": "2021-04-22T15:52:32.610136Z"
    }
   },
   "outputs": [],
   "source": [
    "# use managed tables\n",
    "\n",
    "spark.sql(\"USE default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:47:04.574211Z",
     "start_time": "2021-04-22T15:47:04.451577Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.listColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:51:21.863816Z",
     "start_time": "2021-04-22T15:51:21.676507Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM userrecommendationstable\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:52:07.305416Z",
     "start_time": "2021-04-22T15:52:07.274974Z"
    }
   },
   "outputs": [],
   "source": [
    "# it is not managed so ...\n",
    "\n",
    "# spark.catalog.listColumns('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:52:11.549994Z",
     "start_time": "2021-04-22T15:52:11.482556Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM users\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:52:12.799695Z",
     "start_time": "2021-04-22T15:52:12.731282Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM books\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T16:00:09.457624Z",
     "start_time": "2021-04-22T16:00:09.451488Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The recommended books for user ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Additional exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given the current status of this notebook, redo its content such that major tasks are split into various notebooks, ou Python modules. The purpose is to modularize code having in mind the setup of a *real* recommender system. That is:\n",
    "\n",
    "- A downloader module, focussing on downloading data, cleasing it, and then storing it in a data store. \n",
    "- A recommender module, to create a recommendation module and to pre-compute recommendations in order to save them a data store.\n",
    "- A recommender server, to retrieve recommendations upon queries made to the data store. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Learning Spark - Lightning-Fast Data Analytics, 2nd Ed. J. Damji, B. Wenig, T. Das, and D. Lee. O'Reilly, 2020\n",
    "* Spark: The Definitive Guide - Big Data Processing Made Simple, 1st Ed. B. Chambers and M. Zaharia. O'Reilly, 2018\n",
    "* http://spark.apache.org/docs/latest/ml-guide.html\n",
    "* https://docs.python.org/3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "249px",
    "width": "332px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.98px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
